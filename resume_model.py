# -*- coding: utf-8 -*-
"""resume_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tZzWlFMv21nGYeEp000E9rIQp8ehV4gN
"""

!pip install numpy==1.25.2

!pip install scipy --upgrade

!pip install sentence-transformers

!pip install joblib

!pip install transformers

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import spacy
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

dataset = pd.read_csv('UpdatedResumeDataSet.csv')

dataset.head()

dataset.columns

dataset.describe()

dataset.info()

dataset[dataset["Category"].isnull()]

dataset[dataset["Resume"].isnull()]

dataset.isnull().any()

nlp = spacy.load("en_core_web_sm")

dataset["Resume"].head(20)

def clean_text(text):
    # Basic cleanup
    text = text.lower()
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9., ]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

dataset["Cleaned resume"] = dataset["Resume"].apply(clean_text)

dataset.head()

dataset.drop(labels = ["Resume"], axis=1, inplace = True)

dataset.head(20)

dataset.to_csv('cleaned_resumes.csv', index=False)

jobs_dict = {
    "job_titles":["Data Scientist", "Machine Learning Engineer", "Data Analyst", "Backend Developer", "AI Research Intern", "Cloud DevOps Engineer", "Frontend React Developer", "Software Engineer (Full Stack)", "Product Data Scientist", "AI Prompt Engineer"],
    "job_descriptions":["We're seeking a Data Scientist proficient in Python, machine learning, and statistical modeling. Responsibilities include building predictive models, analyzing large datasets, and presenting insights. Experience with Pandas, Scikit-learn, and SQL required. 2+ years of experience preferred.",
                        "Looking for an ML Engineer with deep knowledge of TensorFlow or PyTorch. You'll build, optimize, and deploy ML models in production. Required: Python, APIs, cloud platforms (AWS or GCP). Strong software engineering fundamentals are a plus.",
                        "Join our team as a Data Analyst to transform raw data into actionable insights. Skills in Excel, SQL, Tableau, and Python are important. Understanding of business metrics and good communication is key.",
                        "Hiring a backend developer experienced in Node.js, REST APIs, and PostgreSQL. You'll design robust APIs and integrate with microservices. Knowledge of Docker and CI/CD pipelines is a bonus.",
                        "Assist in NLP and computer vision research. Looking for students or recent grads with experience in PyTorch, Hugging Face, and academic writing. Strong math/ML foundations are essential.",
                        "Manage and automate cloud infrastructure using AWS, Terraform, and Jenkins. Must be proficient in shell scripting and monitoring tools like Prometheus or Grafana.",
                        "React developer needed to build interactive web apps. Must know JavaScript, React, Redux, and responsive design. Familiarity with Figma or design tools is a plus.",
                        "Build and maintain full-stack applications using Python (Django or Flask) and React. Database knowledge (PostgreSQL/MySQL) and Git workflows required.",
                        "Work cross-functionally to analyze user behavior and experiment with product features. Tools: SQL, Python, A/B testing platforms. Experience with product analytics (Mixpanel, Amplitude) is beneficial.",
                        "Design and test prompts for large language models like GPT-4. Should understand NLP tasks, LLM APIs, and prompt engineering best practices. Creative writing or UX experience is a bonus."]
}

dataset_jobs = pd.DataFrame(jobs_dict)

dataset_jobs.head()

dataset_jobs.to_csv('jobs.csv', index=False)

dataset.info()

dataset_jobs.info()

resumes = dataset["Cleaned resume"].tolist()
texts = dataset_jobs["job_descriptions"].tolist()

model = SentenceTransformer('all-MiniLM-L6-v2')

pairs = []

for i, resume in tqdm(enumerate(resumes)):
    for j, text in enumerate(texts):
        resume_emb = model.encode(resume, convert_to_tensor=True)
        text_emb = model.encode(text, convert_to_tensor=True)
        similarity = util.cos_sim(resume_emb, text_emb).item()
        match_score = round(similarity * 100, 2)  # Scale to 0–100%
        pairs.append((resume, text, match_score))

dataset_similarity  = pd.DataFrame(pairs, columns=["Resume", "Job", "Match Score"])
dataset_similarity.to_csv('similarity_scores.csv', index=False)

dataset_similarity.head(20)

"""#ML model"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns

new_df = pd.read_csv('similarity_scores.csv')

new_df.head()

resume_embeddings = model.encode(new_df['Resume'].tolist(), batch_size=16, convert_to_numpy=True)
job_embeddings = model.encode(new_df['Job'].tolist(), batch_size=16, convert_to_numpy=True)

resume_embeddings.shape

job_embeddings.shape

print(resume_embeddings)

print(job_embeddings)

X = np.hstack((resume_embeddings, job_embeddings))
y = new_df['Match Score'].values

X.shape

y.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

regressor = GradientBoostingRegressor()
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MSE: ", mse)

print("R-score: ", r2)

plt.scatter(y_test, y_pred, alpha=0.6)
plt.xlabel("Actual Match Score")
plt.ylabel("Predicted Match Score")
plt.title("True vs. Predicted Match Scores")
plt.plot([0, 100], [0, 100], 'r--')
plt.grid(True)
plt.show()

errors = y_test - y_pred
sns.histplot(errors, bins=20, kde=True)
plt.title("Prediction Error Distribution")
plt.xlabel("Error (Actual - Predicted)")
plt.show()

for i in range(5):
    print(f"Resume: {new_df.iloc[i]['Resume'][:200]}...")
    print(f"Job: {new_df.iloc[i]['Job'][:200]}...")
    print(f"Actual Score: {y_test[i]:.2f} — Predicted Score: {y_pred[i]:.2f}")
    print("-" * 60)

import shap
explainer = shap.Explainer(regressor, X_train)
shap_values = explainer(X_test[:50])  # use a subset for speed

# Plot feature importance
shap.plots.beeswarm(shap_values)

import joblib
import os

# Create the 'models' directory if it doesn't exist
os.makedirs('models', exist_ok=True)

joblib.dump(regressor, 'models/resume_matcher.pkl')